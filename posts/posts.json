[
  {
    "path": "posts/parquet_com_R/",
    "title": "Parquet para grandes conjuntos de dados no R",
    "description": "Apesar do formato csv ser largamente utilizado, quando o conjuntos de dados é grande, o csv apresenta limitações. Nesses casos, pode ser vantajoso optar pelo Parquet, um  formato de arquivo para armazenamento de dados orientado a colunas. Neste post abordamos o uso de arquivos Parquet no R através da biblioteca Arrow da Apache Foundation, e os microdadosdo Censo Escolar 2019, disponibilizados pelo INEP - Ministério da Educação do Brasil serviram como exemplo.",
    "author": [
      {
        "name": "Jonatas Silva do Espirito Santo",
        "url": "https://github.com/jonates/"
      }
    ],
    "date": "2021-02-15",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nMesmo depois de décadas de sua implementação, é rotina entre as intituições, sejam do setor privado, seja nas instituições públicas, armazenar dados em um arquivo csv (“comma-separated-values”), que como próprio significado da sigla já diz, é um arquivo com valores separados por vírgulas. Vale ressaltar que, como no Brasil a vírgula é utilizada como separador de la decimal, é prática comum utilizar o sinal ponto e vírgula (;) como separador de valores, em vez de usar o sinal vírgula. Também, pode-se utilizar outro caractere como separador de colunas, e, assim o csv passa a ser chamado por alguns como “character-separated-values”.\r\nPor ser bastante difundido e de fácil manipulação (a depender do tamanho do arquivo, é possível abrir em um bloco de notas e consultar/editar o arquivo de dados csv), a maioria dos softwares e aplicativos, tem suporte para leitura e gravação de arquivos csv.\r\nNesses tipos de arquivos, cada linha representa um registro, e os valores dos campos associados a esses registros são separados por vírgula (ou ponto e vírgula ou outro caractere). Por exemplo, suponha que seja de interesse, armazenar em um arquivo csv com informações básicas das últimas 5 copas do mundo de futebol masculino da FIFA. Poderia simplesmente abrir um bloco de notas, digitar o texto abaixo e depois salvar com a extensão .csv:\r\nsede,campeão,ano\r\nRússia,França,2018\r\nBrasil,Alemanha,2014\r\nÁfrica do Sul,Espanha,2010\r\nAlemanha,Itália,2006\r\nCoréia do Sul e Japão,Brasil,2002\r\n\r\nPorém, apesar das diversas vantagens, quando o número de registros e de campos aumentam de forma considerável, a leitura desses arquivos passam a ter custo computacional elevado, pois o csv é um arquivo “row oriented dataset” (ou ainda chamado também de “row storage”), ou seja, como o próprio nome define, é conjunto de dados orientados a linha, e assim, a leitura do arquivo é feito linha a linha. Portanto, mesmo que o interesse de análise seja de uma ou de algumas colunas específicas, todas as colunas serão lidas, diminuindo assim a performance.\r\nEntão, o formato de arquivos Apache parquet, que é um “column oriented dataset”, ou seja, conjunto de dados orientado a colunas, se torna uma excelente alternativa ao .csv para gravar grandes conjuntos de dados para análise, pois, além de leitura mais rápida, a taxa de compressão desses arquivos é muito maior em relação aos .csv ocupando menos espaço em disco.\r\nO conjunto de dados de exemplo\r\nVamos utilizar como exemplo, dados do censo escolar 2019, realizado pelo Ministério da Educação do Brasil. Os microdados, são disponibilizados para download no site do INEP - Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira.\r\nO download é de um arquivo microdados_educacao_basica_2019 compactado com extensão .zip, cujo conteúdo é uma pasta microdados_educacao_basica_2019contendo quatro outras pastas ANEXOS, DADOS, FILTROS, LEIA-ME.\r\nNa pasta DADOS existem 15 conjuntos de dados em formato .csv.\r\nPor exemplo, o arquivo MATRICULA_NORDESTE.CSV que é o conjunto de dados com informações de cada aluno matriculado na educação básica em escolas do Nordeste do Brasil, possui 15.304.589 registros e 103 campos, e, ocupa em disco 3.790.278 KB, ou seja quase 3,8 Gigabytes.\r\nO equipamento\r\nPara os testes de performance de leitura e escrita de arquivos csv e parquetapresentando aqui neste post, foi utilizado um notebook lenovo, com processador intel core i7-4500U quad-core, , 16Gb de memória RAM e HD da WD com capacidade de 1 Tb.\r\nCarregando os arquivos csv no ambiente R\r\nComo os microdados do Censo Escolar, sao arquivos csv com as colunas separadas pelo caracter pipe (|), Eles foram carregados no R com a utilização da função read_delim da biblioteca readr:\r\nmatricula_NE <- readr::read_delim(\r\n  file = \"./microdados_educacao_basica_2019/DADOS/MATRICULA_NORDESTE.CSV\",\r\n  delim = \"|\"\r\n)\r\nSegue abaixo algumas estatísticas relacionadas aos datasets do censo escolar disponibilizados pelo INEP\r\nTabela 1. Informações sobre leituras de arquivos do Censo escolar 2019 no formato csv com uso da biblioteca arrow no R. dataset | registros | campos | tamanho (KB) | tempo de leitura (segundos) —————— | ——-: | :—-: | ———-: | —————: DOCENTES_CO | 892.665 | 136 | 285.874 | 18,49 DOCENTES_NORDESTE | 3.044.057 | 136 | 964.596 | 66,19 DOCENTES_NORTE | 1.067.901 | 136 | 341.999 | 22,83 DOCENTES_SUDESTE | 4.653.852 | 136 | 1.478.987 | 84,88 DOCENTES_SUL | 1.879.896 | 136 | 600.905 | 37,47 ESCOLAS | 228.521 | 234 | 108.590 | 9,63 GESTOR | 187.740 | 83 | 41.668 | 3,39 TURMAS | 2.432.438 | 79 | 450.988 | 30,79 MATRICULA_CO | 3.945.797 | 103 | 975.562 | 71,89 MATRICULA_NORDESTE | 15.304.589 | 103 | 3.790.278 | 2.351,71 MATRICULA_NORTE | 5.198.366 | 103 | 1.289.161 | 77,32 MATRICULA_SUDESTE | 19.785.845 | 103 | 4.803.374 | 4.740,87 MATRICULA_SUL | 6.932.126 | 103 | 1.703.439 | 142,68\r\nVale ressaltar que o readr não conseguiu ler o arquivo MATRICULA_NORDESTE, MATRICULA_SUDESTE e MATRICULA_SUL, e assim, a leitura foi feita com a biblioteca arrow através da função read_delim_arrow, cujo argumentos são os mesmos da read_delim do readr.\r\nSalvando dados em arquivos Parquet com Apache Arrow no R\r\nExiste uma biblioteca no R, de nome arrow, que possui duas funções de fácil utilização, para leitura e escrita de arquivos no formato parquet, respectivamente read_parquet e write_parquet. Para ter acesso a essas funções, primeiro instale a biblioteca arrow:\r\ninstall.packages(\"arrow\")\r\nCom a biblioteca arrow já devidamente instalada, para gravar o dataframe disponível em memória em um arquivo parquet no disco, passe como argumento x da função write_parquet o nome do dataframe que deseja salvar, e no argumento sink o diretório e o nome do arquivo parquet:\r\narrow::write_parquet( \r\n  x = matricula_NE,\r\n  sink = \"./microdados_educacao_basica_2019/DADOS/MATRICULA_NORDESTE.parquet\"\r\n)\r\nPor exempplo, para o arquivo MATRICULA_NORDESTE.CSV, O R demorou 1.227,41 segundos para gravar o dataframe em um arquivo parquet no disco. E, o arquivo parquet ocupou em disco 2.008.247 KB, ou seja um compressão de 47,02%.\r\nSegue abaixo algumas estatísticas relacionadas a gravação em disco de dataframe em formato parquet:\r\nTabela 2. Informações sobre escritas de arquivos do Censo escolar 2019 no formato parquet com uso da biblioteca arrow no R. dataset | registros | campos | tamanho do csv (KB) | tamanho do parquet (KB) | Compressão | tempo de gravação (segundos) —————— | ———:| :—-: | ———-: | ———:| :——: | ——-: DOCENTES_CO | 892.665 | 136 | 285.874 | 31.213 | 89,1% | 4,91 DOCENTES_NORDESTE | 3.044.057 | 136 | 964.596 | 122.796 | 87,3% | 15,16 DOCENTES_NORTE | 1.067.901 | 136 | 341.999 | 41.464 | 87,9% | 5,89 DOCENTES_SUDESTE | 4.653.852 | 136 | 1.478.987 | 151.313 | 89,8% | 24,14 DOCENTES_SUL | 1.879.896 | 136 | 600.905 | 78.500 | 86,9% | 11,20 ESCOLAS | 228.521 | 234 | 108.590 | 16.943 | 84,4% | 9,63 GESTOR | 187.740 | 83 | 41.668 | 10.341 | 75,2% | 3,39 TURMAS | 2.432.438 | 79 | 450.988 | 66.316 | 85,3% | 8,54 MATRICULA_CO | 3.945.797 | 103 | 975.562 | 323.494 | 66,8% | 20,43 MATRICULA_NORDESTE | 15.304.589 | 103 | 3.790.278 | 916.171 | 75,8% | 255,61 MATRICULA_NORTE | 5.198.366 | 103 | 1.289.161 | 315.049 | 75,6% | 17,50 MATRICULA_SUDESTE | 19.785.845 | 103 | 4.803.374 | 1.146.767 | 76,1% | 1.389,93 MATRICULA_SUL | 6.932.126 | 103 | 1.703.439 | 409.738 | 75,9% | 24,07\r\nLendo arquivos Parquet com Apache Arrow no R\r\nAgora, se deseja ler um arquivo parquet salvo em disco, passe como argumento file da função read_parquet o diretório e o nome do arquivo parquet:\r\nmatricula_NE_Parquet <- arrow::read_parquet(\r\n  file = \"./microdados_educacao_basica_2019/DADOS/MATRICULA_NORDESTE.parquet\"\r\n)\r\nSegue abaixo algumas estatísticas relacionadas a leituras dos datasets do censo escolar 2019 salvos em formato parquet, comparando com os gravados em csv:\r\nTabela 3. Informações sobre leituras de arquivos do Censo escolar 2019 no formato parquet com uso da biblioteca arrow no R. dataset | registros | campos | tempo de leitura do csv (segundos) | tempo de leitura do parquet (segundos) | Tempo de Leitura CSV / Tempo de Leitura Parquet —————— | ——-: | :—-: | ———-: | ——-: | ——-: DOCENTES_CO | 892.665 | 136 | 18,49 | 3,03 | 6,10 DOCENTES_NORDESTE | 3.044.057 | 136 | 66,19 | 9,97 | 6,64\r\nDOCENTES_NORTE | 1.067.901 | 136 | 22,83 | 4,14 | 5,51 DOCENTES_SUDESTE | 4.653.852 | 136 | 84,88 | 13,58 | 6,25 DOCENTES_SUL | 1.879.896 | 136 | 37,47 | 7,56 | 4,96 ESCOLAS | 228.521 | 234 | 9,63 | 2,31 | 4,17 GESTOR | 187.740 | 83 | 3,39 | 1,77 | 1,92 TURMAS | 2.432.438 | 79 | 30,79 | 7,68 | 4,01 MATRICULA_CO | 3.945.797 | 103 | 71,89 | 14,02 | 5,13 MATRICULA_NORDESTE | 15.304.589 | 103 | 2.351,71 | 84,27 | 27,91 MATRICULA_NORTE | 5.198.366 | 103 | 77,32 | 14,27 | 5,42 MATRICULA_SUDESTE | 19.785.845 | 103 | 4.740,87 | 243,47 | 19,47 MATRICULA_SUL | 6.932.126 | 103 | 142,68 | 17,80 | 8,02\r\nVale ressaltar que, se for de interesse ler somente algumas colunas do conjunto de dados, ao usar a função read_parquetda biblioteca arrow, basta incluir o argumento col_select e passar para ele um vetor com os nomes das colunas que deseja ler. Esse argumento também suporta as funções auxiliares de seleção de atributos da função selectda biblioteca dplyr como a starts_with(), ends_with(), contains(), matches(), etc.\r\nConsiderações importantes\r\nAo comparar conjuntos de dados identicos salvos em formatos diferentes (csv e parquet), percebe-se (vide tabela 2) que os arquivos parquet consomem muito menos espaços em disco, visto que os arquivos de armazenamento orientados a colunas tem taxa de compressão maiores do que os orientados a linhas. Isso traz uma vantagem imensa em armazenar dados em formatos parquet pois o custo cai significativamente. Pela comparação dos 15 conjuntos de dados do Censo Escolar 2019, verificamos redução média de 81,2% comparando o tamanho de arquivos parquetem relação ao formato csv.\r\nOutro fato que chama atenção é que, mesmo quando se tem conjunto de dados com os mesmo registros e colunas porém salvos em formato diferentes, a leitura de arquivos parquet é mais rápida que de arquivos csv, este último chegando a ter leitura quase 30 vezes mais demorada.\r\nAinda, vale destacar que para tarefas, onde na maioria das vezes não são utilizadas todas as colunas do conjunto de dados para conduzir uma análise, o fato do parquet ser um formato de arquivo orientado a colunas, pode ser lido somente as colunas de interesse, diminuindo assim o tempo de leitura do arquivo, bem como consumindo menos recursos computacional, levando novamente vantagem em relação ao csv que teria que varrer todo o conjunto de dados para depois manter somente as colunas de interesse, resultando em gasto de tempo e de recurso computacional.\r\n\r\n\r\n\r\n",
    "preview": "posts/parquet_com_R/parquet.png",
    "last_modified": "2021-02-15T23:49:59-03:00",
    "input_file": {},
    "preview_width": 450,
    "preview_height": 112
  },
  {
    "path": "posts/spark_com_R/",
    "title": "Big Data com Spark e R: primeiros passos",
    "description": "O Spark é um framework da Apache foundation para trabalhar com processamento de big data através de computação distribuida. Atualmente existem duas bibliotecas que permite escrever jobs para o Spark, em linguagem R, a `sparkr` e a `sparklyr`, esta última desenvolvida e mantida pela RStudio. Este post apresenta desde a instalação do Spark até algumas tarefas de manipulação e processamento de dataset no Spark, e o mais legal, tudo direto do RStudio, utilizando linguagem R, através da biblioteca `sparklyr. Os dados do Censo Escolar 2019 foram utilizados como exemplo.",
    "author": [
      {
        "name": "Jonatas Silva do Espirito Santo",
        "url": "https://github.com/jonates/"
      }
    ],
    "date": "2021-02-14",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\n\r\nPreparando o ambiente\r\nAbra o RStudio e instale a biblioteca sparklyr:\r\ninstall.packages(\"sparklyr\")\r\nO Spark exige que tenha o java 8 instalado em seu computador (a partir da versão 3 do Spark, exige versão 8u92 ou superior do JDK - Java™ SE Development Kit 8, Update 92). Se ainda não possui o java instalado, faça o download da versão exigida pelo Spark no site https://www.oracle.com/br/java/technologies/javase/javase-jdk8-downloads.html.\r\nPara verificar se tem o java correto está instalado através do comando:\r\nsystem(\"java -version\")\r\nEm seguida, após garantir que o computador tem o java 8 instalado, utilize o comando a seguir para a instalar o Spark em seu computador:\r\nsparklyr::spark_install()\r\nAgora, verifique se o Spark está instalado, e em qual versão, através do script a seguir:\r\nsparklyr::spark_installed_versions()\r\nConectando o RStudio ao Spark\r\nCom o Spark já está instalado no computador, primeiro deve ser criado uma conexão entre o RStudio e o Spark através da função spark_connect. Vamos utilizar o Spark no modo “standalone”, portanto no argumento masterutilize o termo “local”:\r\nconexao_spark <- sparklyr::spark_connect(master = \"local\")\r\nAgora, vamos utilizar a função spark_connection_is_open para verificar se a conexão com o Spark está ativa. Ao rodar o comando abaixo é espero retorno TRUE:\r\nsparklyr::spark_connection_is_open(sc = conexao_spark)\r\nTambém, é possível monitorar, através de um browser, todos os jobs que estão sendo executados em seu Spark. Basta executar no RStudio o seguinte comando:\r\nsparklyr::spark_web()\r\nCopiando arquivos do ambiente R para o Spark\r\nPara aproveitar todo ferramental do Spark em realizações de tarefas com um conjunto de dados, seja ela de transformação, contagem ou até mesmo de ajuste de um modelo de machine learning, é preciso copiar os dados para o Spark. Se os dados já estiver carregado no ambiente do RStudio, basta utilizar a função copy_to, passando o nome da conexão no argumento dest, o nome do objeto que deseja copiar para o Spark no argumento df e um nome (entre aspas duplas) que deseja atribuir ao objeto dentro do ambiente Spark no argumento name.\r\nPara ilustrar, vamos utilizar os dados do ideb (índice de desenvolvimento da educação básica) das séries iniciais do ensino fundamental de escolas Brasil, que será carregado através do pacote idebr (para instalar a biblioteca idebr execute o comando devtools::install_github(\"jonates/idebr\")):\r\n#carregando o conjuntos de dados no ambiente R\r\n\r\nideb_escolas <- idebr::ideb_fundamental_iniciais_escolas\r\n\r\n#copiando o conjunto de dados para o Spark\r\n\r\nsparklyr::copy_to(\r\n  dest = conexao_spark,\r\n  df = ideb_escolas,\r\n  name = \"ideb_EFSI\"\r\n)\r\nPara verificar, quais tabelas / conjuntos de dados estão disponíveis em seu Spark, use a função src_tbls() da biblioteca dplyrinformando o nome da conexão Spark no argumento x conforme script a seguir:\r\ndplyr::src_tbls(x = conexao_spark)\r\nLendo e escrevendo arquivos em disco com Apache Spark através do R\r\nPorém, suponha agora que seja de interesse que o Spark leia um arquivo csv direto de um diretório. Para isso, basta utilizar a função spark_read_csv da biblioteca sparklyr, sendo que nos argumentos sctem que colocar o nome da conexão, em name o nome que deseja atribuir ao dataset dentro do Spark, em path o diretório e nome do arquivo csv que deseja ler, em charset está associado com a região e resolve problemas de encoding, em delimiter coloca o caractere que separa as colunas do arquivo csv:\r\nDOCENTES_CO <- sparklyr::spark_read_csv(\r\n  sc = conexao_spark,\r\n  name = \"DOCENTES_CO\",\r\n  path = \"./microdados_educacao_basica_2019/DADOS/DOCENTES_CO.CSV\",\r\n  charset = \"Latin1\",\r\n  delimiter = \"|\",\r\n  memory = FALSE\r\n)  \r\n\r\n\r\n\r\n",
    "preview": "posts/spark_com_R/spark_logo.jpg",
    "last_modified": "2021-02-15T23:32:03-03:00",
    "input_file": {}
  },
  {
    "path": "posts/idebr/",
    "title": "Dados do ideb direto do R?",
    "description": "Com o idebr é possível acessar, de forma fácil e leve, dados do índice de desenvolvimento da educação básica, diretamente do R, sem precisar fazer download de nenhum arquivo.",
    "author": [
      {
        "name": "Jonatas Silva do Espirito Santo",
        "url": "https://github.com/jonates/"
      }
    ],
    "date": "2021-01-11",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nidebr \r\nEste pacote disponibiliza conjuntos de dados, com informações do IDEB - índice de desenvolvimento da educação básica, com as devidas desagregações e estratificações, já no formato tidy, apropriado para geração de visualizações em ggplot2 e que podem serem utilizados em diversas aplicações no R.\r\nAtualmente disponibiliza 6 conjuntos de dados:\r\nideb_fundamental_iniciais_brasil\r\nideb_fundamental_iniciais_escolas\r\nideb_fundamental_finais_brasil\r\nideb_fundamental_finais_escolas\r\nideb_ensino_medio_brasil\r\nideb_ensino_medio_escolas\r\nO ideb_fundamental_iniciais_brasil contem como atributo os valores dos indicadores utilizados para o calculo do IDEB das séries iniciais (1º ao 5º ano) do ensino fundamental para o Brasil, como as taxas de aprovacoes (por serie) e as notas da prova SAEB. Conta tambem com o valor do IDEB e as metas para os anos de 2005, 2007, 2009, 2013, 2015, 2017 e 2019. Tambem estratifica essas informacoes pela rede de ensino.\r\nO ideb_fundamental_iniciais_escolas contem como atributo os valores dos indicadores utilizados para o calculo do IDEB das series iniciais (1º ao 5º ano) do ensino fundamental para as escolas do Brasil, como as taxas de aprovacoes (por serie) e as notas da prova SAEB. Conta tambem com o valor do IDEB e as metas para os anos de 2005, 2007, 2009, 2013, 2015, 2017 e 2019. Tambem estratifica essas informacoes pela rede de ensino.\r\nO ideb_fundamental_finais_brasil contem como atributo os valores dos indicadores utilizados para o calculo do IDEB das séries finais (6º ao 9º ano) do ensino fundamental para o Brasil, como as taxas de aprovacoes (por serie) e as notas da prova SAEB. Conta tambem com o valor do IDEB e as metas para os anos de 2005, 2007, 2009, 2013, 2015, 2017 e 2019. Tambem estratifica essas informacoes pela rede de ensino.\r\nO ideb_fundamental_finais_escolas contem como atributo os valores dos indicadores utilizados para o calculo do IDEB das series finais (6º ao 9º ano) do ensino fundamental para as escolas do Brasil, como as taxas de aprovacoes (por serie) e as notas da prova SAEB. Conta tambem com o valor do IDEB e as metas para os anos de 2005, 2007, 2009, 2013, 2015, 2017 e 2019. Tambem estratifica essas informacoes pela rede de ensino.\r\nO ideb_ensino_medio_brasil contem como atributo os valores dos indicadores utilizados para o calculo do IDEB do ensino medio para o Brasil, como as taxas de aprovacoes (por serie) e as notas da prova SAEB. Conta tambem com o valor do IDEB e as metas para os anos de 2017 e 2019. Tambem estratifica essas informacoes pela rede de ensino.\r\nO ideb_ensino_medio_escolas contem como atributo os valores dos indicadores utilizados para o calculo do IDEB do ensino medio para as escolas do Brasil, como as taxas de aprovacoes (por serie) e as notas da prova SAEB. Conta tambem com o valor do IDEB e as metas para os anos de 2017 e 2019. Tambem estratifica essas informacoes pela rede de ensino.\r\nInstalacao\r\n#Instale o pacote devtoools caso ainda não tenha instalado\r\n#install.packages(devtools)\r\n\r\n#instala o pacote idebr\r\ndevtools::install_github(\"jonates/idebr\")\r\nCarregar conjunto de dados\r\nApos a instalacao do pacote idebr para carregar um conjunto de dados basta digitar no script idebr::nomedodataset. Caso esteja no RStudio, basta digitar ideb:: e apertar a tecla tab que o autocompletar do RStudio vai mostrar todos os conjuntos de dados disponiveis no pacote idebr para que voce possa escolher, conforme imagem abaixo:\r\n\r\nExemplo 1\r\n#carrega a base do ideb do ensino médio do Brasil\r\ndf <- idebr::ideb_ensino_medio_brasil\r\n\r\n#Plot o gráfico do IDEB do Brasil, por ano e por rede de ensino.\r\nlibrary(ggplot2)\r\nggplot(data = df) +\r\n  geom_point(aes(x=ano,y=ideb,colour=rede))\r\n\r\n\r\nRoadmap\r\nEstrato\r\nFundamental - iniciais\r\nFundamental - finais\r\nEnsino Medio\r\nBrasil\r\nOk\r\nOk\r\nOk\r\nRegioes\r\n-\r\n-\r\n-\r\nUF\r\n-\r\n-\r\n-\r\nMunicipios\r\n-\r\n-\r\n-\r\nEscolas\r\nOk\r\nOk\r\nOk\r\n\r\n\r\n\r\n",
    "preview": "posts/idebr/idebr.jpeg",
    "last_modified": "2021-02-05T23:27:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/contribua/",
    "title": "Envie seu artigo",
    "description": "Contribua com artigos para o Blog.",
    "author": [
      {
        "name": "SEIDataLab",
        "url": "https://github.com/SEIDataLab"
      }
    ],
    "date": "2021-01-08",
    "categories": [],
    "contents": "\r\n\r\nbody {text-align: justify}\r\nNosso objetivo é divulgar trabalhos e pesquisas no âmbito da Ciência de Dados e dados públicos. Sabemos que a comunidade dos dados é bastante ativa e colaborativa, neste sentido abrimos este espaço para que a comunidade possa contibuir enviando seus artigos. Adoraríamos publicar o seu artigo por aqui!\r\nO contato para envio dos artigos pode ser feito por meio do endereço de email seidatalab@sei.ba.gov.br\r\nComo este blog é totalmente construído em Distill for Markdown, apreciamos artigos que sejam enviados no formato R Markdown. Para dicas em como escrever o artigo em R Markdown sugerimos o cheat sheet da RStudio\r\nMetadados\r\nAlgumas informações sobre metadados são necessárias para a publicação de artigos no blog. Deixamos a seguir um exemplo com os metadados necessários para a publicação no blog:\r\n\r\n---\r\ntitle: \"Título do Artigo\"\r\ndescription: |\r\n  Subtítulo: Breve descrição sobre o artigo.\r\nauthor:\r\n  - name: Nome e Sobrenome do autor \r\n    url: url de endereço do autor (exemplo: linkedin, github ou site pessoal)\r\n    affiliation: vinculação institucional do autor\r\n    affiliation_url: url de endereço da vinculação autor (se houver)\r\ndate: mm-dd-yyyy # data da publicação\r\ncreative_commons: CC BY #licença creative commons\r\nrepository_url: link do repositório (github) do código (se houver)\r\noutput: \r\n  distill::distill_article:\r\n    self_contained: false\r\n---\r\n\r\nNo campo author devem ser colocadas as características do autor ou autores (podem ser incluídos vários autores), bem como sua afiliação. Observe também o campo da licença creative_commons, que marca o artigo como podendo ser compartilhado (todos os posts enviados devem ter uma licença Creative Commons possibilitando o compartilhamento). Finalmente, o repository_url é usado para fornecer links do artigo, base de dados ou códigos hospodados no GitHub.\r\nEstamos aguardando para publicar o seu artigo!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-31T10:56:08-03:00",
    "input_file": {}
  },
  {
    "path": "posts/sobre_o_seidatalab/",
    "title": "Sobre o SeiDataLab",
    "description": "O que é o SEIDataLab?",
    "author": [
      {
        "name": "Rodrigo Cerqueira",
        "url": "https://www.linkedin.com/in/rodrigobcerqueira/"
      }
    ],
    "date": "2021-01-07",
    "categories": [],
    "contents": "\r\n\r\nbody {text-align: justify}\r\nO SEIDataLab é o laboratório de dados da Superintendência de Estudos Econômicos e Sociais da Bahia (SEI), focado em análise de dados e geração de insights, auxiliando decisões de políticas públicas estaduais.\r\nCriado em 2019, como um desdobramento do Grupo de Trabalho (GT) de Business Intelligence, Big Data e Ciência de Dados, publicado na Portaria nº 18 de 05 de setembro de 2019, o SEIDataLab surgiu, a partir da percepção da área técnica de diferentes Coordenações e Diretorias da SEI, acerca das necessidade de sistematizar conceitos, técnicas e procedimentos em Ciência de Dados e atividades correlatas no âmbito da Superintendência.\r\nO objetivo do laboratório é colocar a SEI na vanguarda da geração de insights para a formulação de políticas públicas orientadas a dados e avançar a inserção cada vez maior do Estadao da Bahia na era do Governo 4.0 e gestão pública orientada a dados.\r\nAtualmente o laboratório construiu e administra o primeiro Data Lake, integrado a um Cluster de Processamento Paralelo de grandes bases de dados do Governo do Estado, construído com o que há de mais moderno no atual cenário das ferramentas de Big Data, como Apache Spark, Apache Ranger e Presto.\r\n\r\n\r\n\r\n",
    "preview": "posts/sobre_o_seidatalab/SeiDataLab.png",
    "last_modified": "2021-01-31T10:56:08-03:00",
    "input_file": {},
    "preview_width": 743,
    "preview_height": 292
  }
]
