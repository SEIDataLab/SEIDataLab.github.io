---
title: "Parquet para grandes conjuntos de dados no R"
description: |
 O formato csv, largamente utilizado, tem limitações quando o conjuntos de dados é grande, e o Apache Parquet é uma excelente alternativa. Este post apresenta como trabalhar com parquet que são arquivos orientado a colunas, utilizando R e as bibliotecas arrow e sparklyr. Os dados do Censo Escolar 2019 foram utilizados como exemplo.
preview: parquet.png
author:
  - name: Jonatas Silva do Espirito Santo 
    url: https://github.com/jonates/
    affiliation: Diretor de Pesquisas | SEI - Governo da Bahia
    affiliation_url: www.sei.ba.gov.br
    orcid_id: 0000-0002-1654-0314
date: 01-28-2021
output:
  distill::distill_article:
    self_contained: false
---
<style>
body {
text-align: justify}
</style>

Mesmo depois de décadas de sua implementação, é rotina entre as intituições, sejam do setor privado, seja nas instituições públicas, armazenar dados em um arquivo csv (“comma-separated-values”), que como próprio significado da sigla já diz, é um arquivo com valores separados por vírgulas. Vale ressaltar que, como no Brasil a vírgula é utilizada como separador de la decimal, é prática comum utilizar o sinal ponto e vírgula (;) como separador de valores, em vez de usar o sinal vírgula. Também, pode-se utilizar outro caractere como separador de colunas, e, assim o csv passa a ser chamado por alguns como "character-separated-values".

Por ser bastante difundido e de fácil manipulação (a depender do tamanho do arquivo, é possível abrir em um bloco de notas e consultar/editar o arquivo de dados csv), a maioria dos softwares e aplicativos, tem suporte para leitura e gravação de arquivos csv.

Nesses tipos de arquivos, cada linha representa um registro, e os valores dos campos associados a esses registros são separados por vírgula (ou ponto e vírgula ou outro caractere). Por exemplo, suponha que seja de interesse, armazenar em um arquivo csv com informações básicas das últimas 5 copas do mundo de futebol masculino da FIFA. Poderia simplesmente abrir um bloco de notas, digitar o texto abaixo e depois salvar com a extensão `.csv`:

```
sede,campeão,ano
Rússia,França,2018
Brasil,Alemanha,2014
África do Sul,Espanha,2010
Alemanha,Itália,2006
Coréia do Sul e Japão,Brasil,2002

```
Porém, apesar das diversas vantagens, quando o número de registros e de campos aumentam de forma considerável, a leitura desses arquivos passam a ter custo computacional elevado, pois o csv é um arquivo “row oriented dataset” (ou ainda chamado também de “row storage”), ou seja, como o próprio nome define, é conjunto de dados orientados a linha, e assim, a leitura do arquivo é feito linha a linha. Portanto, mesmo que o interesse de análise seja de uma ou de algumas colunas específicas, todas as colunas serão lidas, diminuindo assim a performance. 

Então, o formato de arquivos Apache parquet, que é um "column oriented dataset”, ou seja, conjunto de dados orientado a colunas, se torna uma excelente alternativa ao `.csv` para gravar grandes conjuntos de dados para análise, pois, além de leitura mais rápida, a taxa de compressão desses arquivos é muito maior em relação aos `.csv` ocupando menos espaço em disco.

## O conjunto de dados de exemplo

Vamos utilizar como exemplo, dados do censo escolar 2019, realizado pelo Ministério da Educação do Brasil. Os microdados, são disponibilizados para download no site do INEP - Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira.

O download é de um arquivo `microdados_educacao_basica_2019` compactado com extensão `.zip`, cujo conteúdo é uma pasta `microdados_educacao_basica_2019`contendo quatro outras pastas `ANEXOS`, `DADOS`, `FILTROS`, `LEIA-ME`. 

Na pasta `DADOS` existem 15 conjuntos de dados em formato `.csv`.

Por exemplo, o arquivo `MATRICULA_NORDESTE.CSV` que é o conjunto de dados com informações de cada aluno matriculado na educação básica em escolas do Nordeste do Brasil, possui 15.304.589 registros e 103 campos, e, ocupa em disco 3.790.278 KB, ou seja  quase 3,8 Gigabytes.

Como esses arquivos estavam separado pelo caracter pipe (`|`), foram carregados no R com a utilização da função `read_delim` da biblioteca `readr`:

``` {.r}
matricula_NE <- readr::read_delim(
  file = "./microdados_educacao_basica_2019/DADOS/MATRICULA_NORDESTE.CSV",
  delim = "|"
)
```

O R demorou 353,08 segundos para ler esse arquivo do disco e deixar disponível em memória em um formato de dataframe.

Segue abaixo algumas estatísticas relacionadas aos datasets do censo escolar disponibilizados pelo INEP

dataset           | registros | campos | tamanho (KB) | tempo de leitura  (segundos)
------------------ | -------: | :----: | ----------: | ---------------:
DOCENTES_CO        |    892.665 | 136    |   285.874 |    18,49
DOCENTES_NORDESTE  |  3.044.057 | 136    |   964.596 |    66,19
DOCENTES_NORTE     |  1.067.901 | 136    |   341.999 |    22,83
DOCENTES_SUDESTE   |  4.653.852 | 136    | 1.478.987 |    84,88
DOCENTES_SUL       |  1.879.896 | 136    |   600.905 |    37,47
ESCOLAS            |    228.521 | 234    |   108.590 |     9,63
GESTOR             |    187.740 |  83    |    41.668 |     3,39
TURMAS             |  2.432.438 |  79    |   450.988 |    30,79
MATRICULA_CO       |  3.945.797 | 103    |   975.562 |    71,89
MATRICULA_NORDESTE | 15.304.589 | 103    | 3.790.278 | 2.351,71
MATRICULA_NORTE    |  5.198.366 | 103    | 1.289.161 |    77,32
MATRICULA_SUDESTE  | 19.785.845 | 103    | 4.803.374 | 4.740,87
MATRICULA_SUL      |  6.932.126 | 103    | 1.703.439 |   142,68

Vale ressaltar que o readr não conseguiu ler o arquivo `MATRICULA_NORDESTE`, `MATRICULA_SUDESTE` e `MATRICULA_SUL`, e assim, a leitura foi feito com a biblioteca `arrow` através da função `read_delim_arrow`, cujo argumentos são os mesmos da `read_delim` do `readr`.

## Apache Parquet 

## Lendo e escrevendo arquivos Parquet com Apache Arrow no R

Existe uma biblioteca no R, de nome `arrow`, que possui duas funções de fácil utilização, para leitura e escrita de arquivos no formato parquet, respectivamente `read_parquet` e `write_parquet`.

Primeiro, abra o R e instale a biblioteca `arrow`:

``` {.r}
install.packages("arrow")
```

Com a biblioteca `arrow` já devidamente instalada, para gravar o dataframe disponível em memória em um arquivo parquet no disco, passe como argumento `x` da função `write_parquet` o nome do dataframe que deseja salvar, e no argumento `sink` o diretório e o nome do arquivo parquet:

``` {.r}
arrow::write_parquet( 
  x = matricula_NE,
  sink = "./microdados_educacao_basica_2019/DADOS/MATRICULA_NORDESTE.parquet"
)
```

Para o arquivo `MATRICULA_NORDESTE.CSV`, O R demorou 1.227,41 segundos para gravar o dataframe em um arquivo parquet no disco. E, o arquivo parquet ocupou em disco 2.008.247 KB, ou seja um compressão de 47,02%.

Segue abaixo algumas estatísticas relacionadas a gravação em disco de dataframe em formato parquet: 

dataset           | registros | campos | tamanho do csv (KB) | tamanho do parquet (KB) | Compressão | tempo de gravação (segundos)
------------------ | ---------:| :----: | ----------: | ---------:| :------: | -------:
DOCENTES_CO        |    892.665 | 136    |   285.874  |    31.213 | 89,1%    |     4,91
DOCENTES_NORDESTE  |  3.044.057 | 136    |   964.596  |   122.796 | 87,3%    |    15,16
DOCENTES_NORTE     |  1.067.901 | 136    |   341.999  |    41.464 | 87,9%    |     5,89
DOCENTES_SUDESTE   |  4.653.852 | 136    | 1.478.987  |   151.313 | 89,8%    |    24,14
DOCENTES_SUL       |  1.879.896 | 136    |   600.905  |    78.500 | 86,9%    |    11,20
ESCOLAS            |    228.521 | 234    |   108.590  |    16.943 | 84,4%    |     9,63
GESTOR             |    187.740 |  83    |    41.668  |    10.341 | 75,2%    |     3,39
TURMAS             |  2.432.438 |  79    |   450.988  |    66.316 | 85,3%    |     8,54
MATRICULA_CO       |  3.945.797 | 103    |   975.562  |   323.494 | 66,8%    |    20,43
MATRICULA_NORDESTE | 15.304.589 | 103    | 3.790.278  |   916.171 | 75,8%    |   255,61
MATRICULA_NORTE    |  5.198.366 | 103    | 1.289.161  |   315.049 | 75,6%    |    17,50
MATRICULA_SUDESTE  | 19.785.845 | 103    | 4.803.374  | 1.146.767 | 76,1%    | 1.389,93
MATRICULA_SUL      |  6.932.126 | 103    | 1.703.439  |   409.738 | 75,9%    |    24,07


Agora, se deseja ler um arquivo parquet salvo em disco, passe como argumento `file` da função `read_parquet` o diretório e o nome do arquivo parquet:

``` {.r}
matricula_NE_Parquet <- arrow::read_parquet(
  file = "./microdados_educacao_basica_2019/DADOS/MATRICULA_NORDESTE.parquet"
)
```

Segue abaixo algumas estatísticas relacionadas a leituras dos datasets do censo escolar 2019 salvos em formato parquet, comparando com os gravados em csv:

dataset           | registros | campos | tempo de leitura do csv  (segundos) | tempo de leitura do parquet  (segundos)
------------------ | -------: | :----: | ----------: | ---------------:
DOCENTES_CO        |    892.665 | 136  |    18,49    |   3,03
DOCENTES_NORDESTE  |  3.044.057 | 136  |    66,19    |   9,97  
DOCENTES_NORTE     |  1.067.901 | 136  |    22,83    |   4,14
DOCENTES_SUDESTE   |  4.653.852 | 136  |    84,88    |  13,58
DOCENTES_SUL       |  1.879.896 | 136  |    37,47    |   7,56
ESCOLAS            |    228.521 | 234  |     9,63    |   2,31
GESTOR             |    187.740 |  83  |     3,39    |   1,77
TURMAS             |  2.432.438 |  79  |    30,79    |   7,68
MATRICULA_CO       |  3.945.797 | 103  |    71,89    |  14,02 
MATRICULA_NORDESTE | 15.304.589 | 103  | 2.351,71    |  84,27
MATRICULA_NORTE    |  5.198.366 | 103  |    77,32    |  14,27
MATRICULA_SUDESTE  | 19.785.845 | 103  | 4.740,87    | 243,47
MATRICULA_SUL      |  6.932.126 | 103  |   142,68    |  17,80

## Lendo e escrevendo arquivos Parquet com Apache Spark no R

